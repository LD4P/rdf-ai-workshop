{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers\n",
    "While spaCy provides good-enough NER cabilitities, the accelerated improvement of NLP models in recent years means that we can use pre-trained models that leverage modern machine approaches. [HuggingFace](https://huggingface.co/), a company specializing in open-source models, provides an easy-to-use Python library for applying these models on text contained in Sinopia's RDF and to available full-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%reload_ext lab_black\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import kglab\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "import requests\n",
    "import helpers\n",
    "import widgets\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sinopia Stage RDF Text DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes = pd.read_json(\"data/stage-text-nodes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>The Rocks were there.</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/a9115a9...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>Guo li Taiwan da xue li xue yuan zhi wu xue xi...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/7494bfb...</td>\n",
       "      <td>Guo li Taiwan da xue li xue yuan zhi wu xue xi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Restrukturyzat͡sii͡a nat͡sionaľnoï ekonomiky U...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/3b8239a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Work for marimba and synthesizer</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/e7e8b3f...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>FalsaFala</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/07779e3...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>The deserving favorite</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/28c0f11...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Training your boxer</td>\n",
       "      <td>\"An informative and fully illustrated manual t...</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/ba38374...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Pride and prejudice</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/f023120...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Warres of Pompey and Caesar</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/c70bcf6...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uprooting, trauma, and confinement</td>\n",
       "      <td>This thesis is a history of psychiatry through...</td>\n",
       "      <td>&lt;https://api.stage.sinopia.io/resource/86893ce...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2061                              The Rocks were there.   \n",
       "1219  Guo li Taiwan da xue li xue yuan zhi wu xue xi...   \n",
       "170   Restrukturyzat͡sii͡a nat͡sionaľnoï ekonomiky U...   \n",
       "533                    Work for marimba and synthesizer   \n",
       "2099                                          FalsaFala   \n",
       "1222                             The deserving favorite   \n",
       "310                                 Training your boxer   \n",
       "476                                 Pride and prejudice   \n",
       "186                         Warres of Pompey and Caesar   \n",
       "4                    Uprooting, trauma, and confinement   \n",
       "\n",
       "                                                summary  \\\n",
       "2061                                               None   \n",
       "1219                                               None   \n",
       "170                                                None   \n",
       "533                                                None   \n",
       "2099                                               None   \n",
       "1222                                               None   \n",
       "310   \"An informative and fully illustrated manual t...   \n",
       "476                                                None   \n",
       "186                                                None   \n",
       "4     This thesis is a history of psychiatry through...   \n",
       "\n",
       "                                                    url  \\\n",
       "2061  <https://api.stage.sinopia.io/resource/a9115a9...   \n",
       "1219  <https://api.stage.sinopia.io/resource/7494bfb...   \n",
       "170   <https://api.stage.sinopia.io/resource/3b8239a...   \n",
       "533   <https://api.stage.sinopia.io/resource/e7e8b3f...   \n",
       "2099  <https://api.stage.sinopia.io/resource/07779e3...   \n",
       "1222  <https://api.stage.sinopia.io/resource/28c0f11...   \n",
       "310   <https://api.stage.sinopia.io/resource/ba38374...   \n",
       "476   <https://api.stage.sinopia.io/resource/f023120...   \n",
       "186   <https://api.stage.sinopia.io/resource/c70bcf6...   \n",
       "4     <https://api.stage.sinopia.io/resource/86893ce...   \n",
       "\n",
       "                                                  label  \n",
       "2061                                               None  \n",
       "1219  Guo li Taiwan da xue li xue yuan zhi wu xue xi...  \n",
       "170                                                None  \n",
       "533                                                None  \n",
       "2099                                               None  \n",
       "1222                                               None  \n",
       "310                                                None  \n",
       "476                                                None  \n",
       "186                                                None  \n",
       "4                                                  None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_text_nodes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface NER Pipeline\n",
    "The HuggingFace transformers library provides very easy-to-use pipelines for running common NLP tasks like NER. We will create a NER pipeline and run the following *summary* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                               Organoselenium chemistry\n",
       "summary    \"Selenium plays an important role in the opera...\n",
       "url        <https://api.stage.sinopia.io/resource/b19dd91...\n",
       "label                                                   None\n",
       "Name: 1244, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_text_nodes.iloc[1244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Selenium plays an important role in the operation of biological processes. Thus, organoselenium compounds are of current interest in chemistry as well as in biology. This book covers a wide section of selenium chemistry. It provides an overview of the synthesis of a variety of organoselenides including selenourea, selenocarbonyls, selenoamides, selenazadienes and Se-containing heterocycles by various approaches such as coupling, C-H activation, radical reactions, and microwave induced reactions. The applications of selenides in biological processes, pharmacology and as reagents and catalysts have been illustrated\"--Page 4 of cover.\n"
     ]
    }
   ],
   "source": [
    "print(stage_text_nodes.iloc[1244].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ner_pipe = pipeline(\"ner\", \"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = ner_pipe(stage_text_nodes.iloc[1244].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'LABEL_1', 'score': 0.62920445, 'index': 1, 'word': '\"', 'start': 0, 'end': 1}\n",
      "{'entity': 'LABEL_1', 'score': 0.5970293, 'index': 2, 'word': 'se', 'start': 1, 'end': 3}\n",
      "{'entity': 'LABEL_1', 'score': 0.681782, 'index': 3, 'word': '##len', 'start': 3, 'end': 6}\n",
      "{'entity': 'LABEL_1', 'score': 0.60181314, 'index': 4, 'word': '##ium', 'start': 6, 'end': 9}\n",
      "{'entity': 'LABEL_1', 'score': 0.5705185, 'index': 5, 'word': 'plays', 'start': 10, 'end': 15}\n",
      "{'entity': 'LABEL_0', 'score': 0.6519011, 'index': 6, 'word': 'an', 'start': 16, 'end': 18}\n",
      "{'entity': 'LABEL_0', 'score': 0.54520696, 'index': 7, 'word': 'important', 'start': 19, 'end': 28}\n",
      "{'entity': 'LABEL_0', 'score': 0.5138432, 'index': 8, 'word': 'role', 'start': 29, 'end': 33}\n",
      "{'entity': 'LABEL_0', 'score': 0.65190136, 'index': 9, 'word': 'in', 'start': 34, 'end': 36}\n",
      "{'entity': 'LABEL_0', 'score': 0.65149987, 'index': 10, 'word': 'the', 'start': 37, 'end': 40}\n",
      "{'entity': 'LABEL_0', 'score': 0.56185436, 'index': 11, 'word': 'operation', 'start': 41, 'end': 50}\n",
      "{'entity': 'LABEL_1', 'score': 0.51182693, 'index': 12, 'word': 'of', 'start': 51, 'end': 53}\n",
      "{'entity': 'LABEL_1', 'score': 0.6018495, 'index': 13, 'word': 'biological', 'start': 54, 'end': 64}\n",
      "{'entity': 'LABEL_1', 'score': 0.60269094, 'index': 14, 'word': 'processes', 'start': 65, 'end': 74}\n",
      "{'entity': 'LABEL_0', 'score': 0.65186054, 'index': 15, 'word': '.', 'start': 74, 'end': 75}\n",
      "{'entity': 'LABEL_1', 'score': 0.54816556, 'index': 16, 'word': 'thus', 'start': 76, 'end': 80}\n",
      "{'entity': 'LABEL_1', 'score': 0.5434417, 'index': 17, 'word': ',', 'start': 80, 'end': 81}\n",
      "{'entity': 'LABEL_1', 'score': 0.54509497, 'index': 18, 'word': 'organ', 'start': 82, 'end': 87}\n",
      "{'entity': 'LABEL_1', 'score': 0.64553195, 'index': 19, 'word': '##ose', 'start': 87, 'end': 90}\n",
      "{'entity': 'LABEL_1', 'score': 0.66894996, 'index': 20, 'word': '##len', 'start': 90, 'end': 93}\n",
      "{'entity': 'LABEL_1', 'score': 0.5608524, 'index': 21, 'word': '##ium', 'start': 93, 'end': 96}\n",
      "{'entity': 'LABEL_1', 'score': 0.5538857, 'index': 22, 'word': 'compounds', 'start': 97, 'end': 106}\n",
      "{'entity': 'LABEL_1', 'score': 0.51752347, 'index': 23, 'word': 'are', 'start': 107, 'end': 110}\n",
      "{'entity': 'LABEL_1', 'score': 0.6854215, 'index': 24, 'word': 'of', 'start': 111, 'end': 113}\n",
      "{'entity': 'LABEL_1', 'score': 0.5907374, 'index': 25, 'word': 'current', 'start': 114, 'end': 121}\n",
      "{'entity': 'LABEL_0', 'score': 0.5169895, 'index': 26, 'word': 'interest', 'start': 122, 'end': 130}\n",
      "{'entity': 'LABEL_1', 'score': 0.53087884, 'index': 27, 'word': 'in', 'start': 131, 'end': 133}\n",
      "{'entity': 'LABEL_1', 'score': 0.5253068, 'index': 28, 'word': 'chemistry', 'start': 134, 'end': 143}\n",
      "{'entity': 'LABEL_0', 'score': 0.6520031, 'index': 29, 'word': 'as', 'start': 144, 'end': 146}\n",
      "{'entity': 'LABEL_0', 'score': 0.65157974, 'index': 30, 'word': 'well', 'start': 147, 'end': 151}\n",
      "{'entity': 'LABEL_0', 'score': 0.52382636, 'index': 31, 'word': 'as', 'start': 152, 'end': 154}\n",
      "{'entity': 'LABEL_0', 'score': 0.6520002, 'index': 32, 'word': 'in', 'start': 155, 'end': 157}\n",
      "{'entity': 'LABEL_1', 'score': 0.53689426, 'index': 33, 'word': 'biology', 'start': 158, 'end': 165}\n",
      "{'entity': 'LABEL_0', 'score': 0.65150034, 'index': 34, 'word': '.', 'start': 165, 'end': 166}\n",
      "{'entity': 'LABEL_1', 'score': 0.6442549, 'index': 35, 'word': 'this', 'start': 167, 'end': 171}\n",
      "{'entity': 'LABEL_0', 'score': 0.5508289, 'index': 36, 'word': 'book', 'start': 172, 'end': 176}\n",
      "{'entity': 'LABEL_1', 'score': 0.5576654, 'index': 37, 'word': 'covers', 'start': 177, 'end': 183}\n",
      "{'entity': 'LABEL_0', 'score': 0.6519454, 'index': 38, 'word': 'a', 'start': 184, 'end': 185}\n",
      "{'entity': 'LABEL_1', 'score': 0.5033875, 'index': 39, 'word': 'wide', 'start': 186, 'end': 190}\n",
      "{'entity': 'LABEL_0', 'score': 0.5350678, 'index': 40, 'word': 'section', 'start': 191, 'end': 198}\n",
      "{'entity': 'LABEL_0', 'score': 0.57480013, 'index': 41, 'word': 'of', 'start': 199, 'end': 201}\n",
      "{'entity': 'LABEL_1', 'score': 0.5940714, 'index': 42, 'word': 'se', 'start': 202, 'end': 204}\n",
      "{'entity': 'LABEL_1', 'score': 0.6483325, 'index': 43, 'word': '##len', 'start': 204, 'end': 207}\n",
      "{'entity': 'LABEL_1', 'score': 0.5834903, 'index': 44, 'word': '##ium', 'start': 207, 'end': 210}\n",
      "{'entity': 'LABEL_1', 'score': 0.5252603, 'index': 45, 'word': 'chemistry', 'start': 211, 'end': 220}\n",
      "{'entity': 'LABEL_0', 'score': 0.65144324, 'index': 46, 'word': '.', 'start': 220, 'end': 221}\n",
      "{'entity': 'LABEL_1', 'score': 0.50023377, 'index': 47, 'word': 'it', 'start': 222, 'end': 224}\n",
      "{'entity': 'LABEL_1', 'score': 0.5115742, 'index': 48, 'word': 'provides', 'start': 225, 'end': 233}\n",
      "{'entity': 'LABEL_1', 'score': 0.5956365, 'index': 49, 'word': 'an', 'start': 234, 'end': 236}\n",
      "{'entity': 'LABEL_0', 'score': 0.64034843, 'index': 50, 'word': 'overview', 'start': 237, 'end': 245}\n",
      "{'entity': 'LABEL_0', 'score': 0.6406758, 'index': 51, 'word': 'of', 'start': 246, 'end': 248}\n",
      "{'entity': 'LABEL_1', 'score': 0.5775509, 'index': 52, 'word': 'the', 'start': 249, 'end': 252}\n",
      "{'entity': 'LABEL_1', 'score': 0.56115544, 'index': 53, 'word': 'synthesis', 'start': 253, 'end': 262}\n",
      "{'entity': 'LABEL_1', 'score': 0.53989756, 'index': 54, 'word': 'of', 'start': 263, 'end': 265}\n",
      "{'entity': 'LABEL_1', 'score': 0.5658797, 'index': 55, 'word': 'a', 'start': 266, 'end': 267}\n",
      "{'entity': 'LABEL_0', 'score': 0.52627003, 'index': 56, 'word': 'variety', 'start': 268, 'end': 275}\n",
      "{'entity': 'LABEL_1', 'score': 0.55336285, 'index': 57, 'word': 'of', 'start': 276, 'end': 278}\n",
      "{'entity': 'LABEL_1', 'score': 0.5360106, 'index': 58, 'word': 'organ', 'start': 279, 'end': 284}\n",
      "{'entity': 'LABEL_1', 'score': 0.6459044, 'index': 59, 'word': '##ose', 'start': 284, 'end': 287}\n",
      "{'entity': 'LABEL_1', 'score': 0.6269189, 'index': 60, 'word': '##len', 'start': 287, 'end': 290}\n",
      "{'entity': 'LABEL_1', 'score': 0.67464334, 'index': 61, 'word': '##ides', 'start': 290, 'end': 294}\n",
      "{'entity': 'LABEL_0', 'score': 0.5894545, 'index': 62, 'word': 'including', 'start': 295, 'end': 304}\n",
      "{'entity': 'LABEL_1', 'score': 0.5767279, 'index': 63, 'word': 'se', 'start': 305, 'end': 307}\n",
      "{'entity': 'LABEL_1', 'score': 0.62736726, 'index': 64, 'word': '##len', 'start': 307, 'end': 310}\n",
      "{'entity': 'LABEL_1', 'score': 0.5737534, 'index': 65, 'word': '##our', 'start': 310, 'end': 313}\n",
      "{'entity': 'LABEL_1', 'score': 0.70861125, 'index': 66, 'word': '##ea', 'start': 313, 'end': 315}\n",
      "{'entity': 'LABEL_0', 'score': 0.6516571, 'index': 67, 'word': ',', 'start': 315, 'end': 316}\n",
      "{'entity': 'LABEL_1', 'score': 0.57722443, 'index': 68, 'word': 'se', 'start': 317, 'end': 319}\n",
      "{'entity': 'LABEL_1', 'score': 0.64840966, 'index': 69, 'word': '##len', 'start': 319, 'end': 322}\n",
      "{'entity': 'LABEL_1', 'score': 0.5863008, 'index': 70, 'word': '##oca', 'start': 322, 'end': 325}\n",
      "{'entity': 'LABEL_0', 'score': 0.50547725, 'index': 71, 'word': '##rb', 'start': 325, 'end': 327}\n",
      "{'entity': 'LABEL_1', 'score': 0.59989107, 'index': 72, 'word': '##ony', 'start': 327, 'end': 330}\n",
      "{'entity': 'LABEL_1', 'score': 0.6692963, 'index': 73, 'word': '##ls', 'start': 330, 'end': 332}\n",
      "{'entity': 'LABEL_0', 'score': 0.6519157, 'index': 74, 'word': ',', 'start': 332, 'end': 333}\n",
      "{'entity': 'LABEL_1', 'score': 0.55317056, 'index': 75, 'word': 'se', 'start': 334, 'end': 336}\n",
      "{'entity': 'LABEL_1', 'score': 0.6236894, 'index': 76, 'word': '##len', 'start': 336, 'end': 339}\n",
      "{'entity': 'LABEL_1', 'score': 0.5837531, 'index': 77, 'word': '##oa', 'start': 339, 'end': 341}\n",
      "{'entity': 'LABEL_1', 'score': 0.5248373, 'index': 78, 'word': '##mide', 'start': 341, 'end': 345}\n",
      "{'entity': 'LABEL_1', 'score': 0.6307075, 'index': 79, 'word': '##s', 'start': 345, 'end': 346}\n",
      "{'entity': 'LABEL_0', 'score': 0.65189844, 'index': 80, 'word': ',', 'start': 346, 'end': 347}\n",
      "{'entity': 'LABEL_0', 'score': 0.6298872, 'index': 81, 'word': 'selena', 'start': 348, 'end': 354}\n",
      "{'entity': 'LABEL_1', 'score': 0.52389747, 'index': 82, 'word': '##za', 'start': 354, 'end': 356}\n",
      "{'entity': 'LABEL_1', 'score': 0.51816744, 'index': 83, 'word': '##die', 'start': 356, 'end': 359}\n",
      "{'entity': 'LABEL_1', 'score': 0.7124533, 'index': 84, 'word': '##nes', 'start': 359, 'end': 362}\n",
      "{'entity': 'LABEL_1', 'score': 0.53312624, 'index': 85, 'word': 'and', 'start': 363, 'end': 366}\n",
      "{'entity': 'LABEL_1', 'score': 0.6030278, 'index': 86, 'word': 'se', 'start': 367, 'end': 369}\n",
      "{'entity': 'LABEL_0', 'score': 0.65152454, 'index': 87, 'word': '-', 'start': 369, 'end': 370}\n",
      "{'entity': 'LABEL_0', 'score': 0.64183927, 'index': 88, 'word': 'containing', 'start': 370, 'end': 380}\n",
      "{'entity': 'LABEL_0', 'score': 0.5182372, 'index': 89, 'word': 'het', 'start': 381, 'end': 384}\n",
      "{'entity': 'LABEL_1', 'score': 0.62197554, 'index': 90, 'word': '##ero', 'start': 384, 'end': 387}\n",
      "{'entity': 'LABEL_1', 'score': 0.61953217, 'index': 91, 'word': '##cycle', 'start': 387, 'end': 392}\n",
      "{'entity': 'LABEL_1', 'score': 0.62774056, 'index': 92, 'word': '##s', 'start': 392, 'end': 393}\n",
      "{'entity': 'LABEL_0', 'score': 0.59096277, 'index': 93, 'word': 'by', 'start': 394, 'end': 396}\n",
      "{'entity': 'LABEL_1', 'score': 0.57661694, 'index': 94, 'word': 'various', 'start': 397, 'end': 404}\n",
      "{'entity': 'LABEL_0', 'score': 0.57809323, 'index': 95, 'word': 'approaches', 'start': 405, 'end': 415}\n",
      "{'entity': 'LABEL_1', 'score': 0.50344425, 'index': 96, 'word': 'such', 'start': 416, 'end': 420}\n",
      "{'entity': 'LABEL_0', 'score': 0.65199834, 'index': 97, 'word': 'as', 'start': 421, 'end': 423}\n",
      "{'entity': 'LABEL_1', 'score': 0.5027766, 'index': 98, 'word': 'coupling', 'start': 424, 'end': 432}\n",
      "{'entity': 'LABEL_1', 'score': 0.539347, 'index': 99, 'word': ',', 'start': 432, 'end': 433}\n",
      "{'entity': 'LABEL_0', 'score': 0.518307, 'index': 100, 'word': 'c', 'start': 434, 'end': 435}\n",
      "{'entity': 'LABEL_0', 'score': 0.65155053, 'index': 101, 'word': '-', 'start': 435, 'end': 436}\n",
      "{'entity': 'LABEL_0', 'score': 0.54644626, 'index': 102, 'word': 'h', 'start': 436, 'end': 437}\n",
      "{'entity': 'LABEL_0', 'score': 0.537899, 'index': 103, 'word': 'activation', 'start': 438, 'end': 448}\n",
      "{'entity': 'LABEL_0', 'score': 0.6519821, 'index': 104, 'word': ',', 'start': 448, 'end': 449}\n",
      "{'entity': 'LABEL_0', 'score': 0.6062765, 'index': 105, 'word': 'radical', 'start': 450, 'end': 457}\n",
      "{'entity': 'LABEL_1', 'score': 0.5344832, 'index': 106, 'word': 'reactions', 'start': 458, 'end': 467}\n",
      "{'entity': 'LABEL_0', 'score': 0.6519052, 'index': 107, 'word': ',', 'start': 467, 'end': 468}\n",
      "{'entity': 'LABEL_1', 'score': 0.5132323, 'index': 108, 'word': 'and', 'start': 469, 'end': 472}\n",
      "{'entity': 'LABEL_1', 'score': 0.52426934, 'index': 109, 'word': 'microwave', 'start': 473, 'end': 482}\n",
      "{'entity': 'LABEL_0', 'score': 0.5486746, 'index': 110, 'word': 'induced', 'start': 483, 'end': 490}\n",
      "{'entity': 'LABEL_1', 'score': 0.5098957, 'index': 111, 'word': 'reactions', 'start': 491, 'end': 500}\n",
      "{'entity': 'LABEL_0', 'score': 0.6508672, 'index': 112, 'word': '.', 'start': 500, 'end': 501}\n",
      "{'entity': 'LABEL_1', 'score': 0.576666, 'index': 113, 'word': 'the', 'start': 502, 'end': 505}\n",
      "{'entity': 'LABEL_0', 'score': 0.59758866, 'index': 114, 'word': 'applications', 'start': 506, 'end': 518}\n",
      "{'entity': 'LABEL_1', 'score': 0.55905014, 'index': 115, 'word': 'of', 'start': 519, 'end': 521}\n",
      "{'entity': 'LABEL_1', 'score': 0.62413365, 'index': 116, 'word': 'se', 'start': 522, 'end': 524}\n",
      "{'entity': 'LABEL_1', 'score': 0.655105, 'index': 117, 'word': '##len', 'start': 524, 'end': 527}\n",
      "{'entity': 'LABEL_1', 'score': 0.63510317, 'index': 118, 'word': '##ides', 'start': 527, 'end': 531}\n",
      "{'entity': 'LABEL_0', 'score': 0.52494246, 'index': 119, 'word': 'in', 'start': 532, 'end': 534}\n",
      "{'entity': 'LABEL_1', 'score': 0.54655117, 'index': 120, 'word': 'biological', 'start': 535, 'end': 545}\n",
      "{'entity': 'LABEL_1', 'score': 0.57300043, 'index': 121, 'word': 'processes', 'start': 546, 'end': 555}\n",
      "{'entity': 'LABEL_0', 'score': 0.65195554, 'index': 122, 'word': ',', 'start': 555, 'end': 556}\n",
      "{'entity': 'LABEL_0', 'score': 0.6013105, 'index': 123, 'word': 'ph', 'start': 557, 'end': 559}\n",
      "{'entity': 'LABEL_0', 'score': 0.6515767, 'index': 124, 'word': '##arm', 'start': 559, 'end': 562}\n",
      "{'entity': 'LABEL_1', 'score': 0.56087637, 'index': 125, 'word': '##aco', 'start': 562, 'end': 565}\n",
      "{'entity': 'LABEL_1', 'score': 0.5386909, 'index': 126, 'word': '##logy', 'start': 565, 'end': 569}\n",
      "{'entity': 'LABEL_1', 'score': 0.5358339, 'index': 127, 'word': 'and', 'start': 570, 'end': 573}\n",
      "{'entity': 'LABEL_1', 'score': 0.53284895, 'index': 128, 'word': 'as', 'start': 574, 'end': 576}\n",
      "{'entity': 'LABEL_1', 'score': 0.54389316, 'index': 129, 'word': 're', 'start': 577, 'end': 579}\n",
      "{'entity': 'LABEL_1', 'score': 0.53067183, 'index': 130, 'word': '##age', 'start': 579, 'end': 582}\n",
      "{'entity': 'LABEL_1', 'score': 0.6002203, 'index': 131, 'word': '##nts', 'start': 582, 'end': 585}\n",
      "{'entity': 'LABEL_1', 'score': 0.513161, 'index': 132, 'word': 'and', 'start': 586, 'end': 589}\n",
      "{'entity': 'LABEL_0', 'score': 0.5258417, 'index': 133, 'word': 'catalyst', 'start': 590, 'end': 598}\n",
      "{'entity': 'LABEL_1', 'score': 0.68609965, 'index': 134, 'word': '##s', 'start': 598, 'end': 599}\n",
      "{'entity': 'LABEL_0', 'score': 0.53673136, 'index': 135, 'word': 'have', 'start': 600, 'end': 604}\n",
      "{'entity': 'LABEL_0', 'score': 0.55227816, 'index': 136, 'word': 'been', 'start': 605, 'end': 609}\n",
      "{'entity': 'LABEL_0', 'score': 0.56947345, 'index': 137, 'word': 'illustrated', 'start': 610, 'end': 621}\n",
      "{'entity': 'LABEL_1', 'score': 0.6121857, 'index': 138, 'word': '\"', 'start': 621, 'end': 622}\n",
      "{'entity': 'LABEL_1', 'score': 0.5008212, 'index': 139, 'word': '-', 'start': 622, 'end': 623}\n",
      "{'entity': 'LABEL_1', 'score': 0.574203, 'index': 140, 'word': '-', 'start': 623, 'end': 624}\n",
      "{'entity': 'LABEL_0', 'score': 0.52382237, 'index': 141, 'word': 'page', 'start': 624, 'end': 628}\n",
      "{'entity': 'LABEL_0', 'score': 0.65195227, 'index': 142, 'word': '4', 'start': 629, 'end': 630}\n",
      "{'entity': 'LABEL_1', 'score': 0.53142244, 'index': 143, 'word': 'of', 'start': 631, 'end': 633}\n",
      "{'entity': 'LABEL_1', 'score': 0.56801444, 'index': 144, 'word': 'cover', 'start': 634, 'end': 639}\n",
      "{'entity': 'LABEL_1', 'score': 0.6335632, 'index': 145, 'word': '.', 'start': 639, 'end': 640}\n"
     ]
    }
   ],
   "source": [
    "for row in ner_results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner2_results = ner_pipe(stage_text_nodes.iloc[310].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'LABEL_1', 'score': 0.5707234, 'index': 1, 'word': '\"', 'start': 0, 'end': 1}\n",
      "{'entity': 'LABEL_1', 'score': 0.5932133, 'index': 2, 'word': 'an', 'start': 1, 'end': 3}\n",
      "{'entity': 'LABEL_1', 'score': 0.5583519, 'index': 3, 'word': 'inform', 'start': 4, 'end': 10}\n",
      "{'entity': 'LABEL_1', 'score': 0.5918726, 'index': 4, 'word': '##ative', 'start': 10, 'end': 15}\n",
      "{'entity': 'LABEL_1', 'score': 0.5740556, 'index': 5, 'word': 'and', 'start': 16, 'end': 19}\n",
      "{'entity': 'LABEL_1', 'score': 0.52536446, 'index': 6, 'word': 'fully', 'start': 20, 'end': 25}\n",
      "{'entity': 'LABEL_0', 'score': 0.52550507, 'index': 7, 'word': 'illustrated', 'start': 26, 'end': 37}\n",
      "{'entity': 'LABEL_0', 'score': 0.5471797, 'index': 8, 'word': 'manual', 'start': 38, 'end': 44}\n",
      "{'entity': 'LABEL_1', 'score': 0.6625897, 'index': 9, 'word': 'that', 'start': 45, 'end': 49}\n",
      "{'entity': 'LABEL_1', 'score': 0.5457408, 'index': 10, 'word': 'covers', 'start': 50, 'end': 56}\n",
      "{'entity': 'LABEL_0', 'score': 0.5877897, 'index': 11, 'word': 'all', 'start': 57, 'end': 60}\n",
      "{'entity': 'LABEL_0', 'score': 0.6510497, 'index': 12, 'word': 'aspects', 'start': 61, 'end': 68}\n",
      "{'entity': 'LABEL_0', 'score': 0.5488727, 'index': 13, 'word': 'of', 'start': 69, 'end': 71}\n",
      "{'entity': 'LABEL_1', 'score': 0.50511426, 'index': 14, 'word': 'training', 'start': 72, 'end': 80}\n",
      "{'entity': 'LABEL_1', 'score': 0.58138406, 'index': 15, 'word': 'for', 'start': 81, 'end': 84}\n",
      "{'entity': 'LABEL_1', 'score': 0.6928722, 'index': 16, 'word': 'your', 'start': 85, 'end': 89}\n",
      "{'entity': 'LABEL_1', 'score': 0.64815843, 'index': 17, 'word': 'pet', 'start': 90, 'end': 93}\n",
      "{'entity': 'LABEL_1', 'score': 0.50055176, 'index': 18, 'word': 'boxer', 'start': 94, 'end': 99}\n",
      "{'entity': 'LABEL_1', 'score': 0.6625064, 'index': 19, 'word': '\"', 'start': 99, 'end': 100}\n",
      "{'entity': 'LABEL_1', 'score': 0.51631856, 'index': 20, 'word': '-', 'start': 100, 'end': 101}\n",
      "{'entity': 'LABEL_1', 'score': 0.59618664, 'index': 21, 'word': '-', 'start': 101, 'end': 102}\n",
      "{'entity': 'LABEL_1', 'score': 0.5340563, 'index': 22, 'word': 'provided', 'start': 103, 'end': 111}\n",
      "{'entity': 'LABEL_1', 'score': 0.5237139, 'index': 23, 'word': 'by', 'start': 112, 'end': 114}\n",
      "{'entity': 'LABEL_0', 'score': 0.5065597, 'index': 24, 'word': 'publisher', 'start': 115, 'end': 124}\n",
      "{'entity': 'LABEL_1', 'score': 0.5888895, 'index': 25, 'word': '.', 'start': 124, 'end': 125}\n"
     ]
    }
   ],
   "source": [
    "for row in ner2_results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results returned from the [HuggingFace][HUG] NER contain more information and are structured differently than [spaCy][SPACY]. In the [HuggingFace][HUG] pipeline, there are only four classes of entities:\n",
    "\n",
    "- `I-PER` for a Person name\n",
    "- `I-ORG` for an Organization name\n",
    "- `I-LOC` for a location\n",
    "- `I-MISC` for a Miscellaneous entity. \n",
    "\n",
    "The HuggingFace NER also gives a statistical score on how the model is confident that it matched an entity. Also, the [HuggingFace][HUG] NER pipeline results does some character masking (seen as `##` in the results) for many of the entities.\n",
    "\n",
    "[HUG]: https://huggingface.co/\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Summarization Pipeline\n",
    "Another [HuggingFace][HUG] pipeline is the *summarization* task that takes a large document and automatically summarizes the text. The pipeline leverages a [Bart](https://arxiv.org/abs/1910.13461) model that was fine-tuned on a CNN / Daily Mail data set.\n",
    "\n",
    "[HUG]: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'PegasusForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", \"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we looked a *Alarmingly suspicious* that is cataloged in Sinopia at https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c. This resource does not have an abstract or BIBFRAME Summary, so let us read in sections of the full-text to this [HuggingFace][HUG] summizer and see if can autogenerate a summary and add it to this RDF graph.\n",
    "\n",
    "[HUG]: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/65a2b059-5ac1-48a6-adbb-870712c3060c.txt\") as fo:\n",
    "    example1_text = fo.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we send the entire full-text to the `summarizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17224 > 512). Running this sequence through the model will result in indexing errors\n",
      "Input length of input_ids is 17224, but ``max_length`` is set to 20. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (17225) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 17225].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary_result \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample1_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:235\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:137\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    142\u001b[0m     ):\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/pipelines/base.py:1043\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/pipelines/base.py:1050\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1049\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1050\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/pipelines/base.py:959\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m    958\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 959\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:159\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m], generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 159\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/generation_utils.py:1288\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1285\u001b[0m         )\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;66;03m# 10. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_sample_gen_mode:\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;66;03m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1303\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1304\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         renormalize_logits\u001b[38;5;241m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1309\u001b[0m     )\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/generation_utils.py:1683\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1683\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1691\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1351\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1351\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1366\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/70-79 Presentations/70.02 RDF-AI Workshop - 02022 LD4 Conference/ld4-env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:984\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    983\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 984\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (17225) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 17225].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "summary_result = summarizer(example1_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of tokens that the `summarizer` pipeline can process at a time is **1024** while our full-text has **20,348** tokens. Let us try breaking down our large text into smaller \"chunks\", send each chunk into the `summarizer` pipeline, capture the resulting summary, and at the end, see if the summaries make sense.\n",
    "\n",
    "First, we will create a list of all of the words in the full-text and then send and summarize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in full-text 10,570\n"
     ]
    }
   ],
   "source": [
    "example1_words = example1_text.split()\n",
    "print(f\"Total words in full-text {len(example1_words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 268, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2022-07-11 15:12:05.838030\n",
      "1 0 to 150 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 474, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 150 to 300 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 323, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 300 to 450 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 257, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 450 to 600 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 365, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 600 to 750 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 293, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 750 to 900 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 298, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 900 to 1,050 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 225, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1,050 to 1,200 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 279, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1,200 to 1,350 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 237, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1,350 to 1,500 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 226, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1,500 to 1,650 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 211, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 1,650 to 1,800 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 272, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 1,800 to 1,950 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 199, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1,950 to 2,100 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 211, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2,100 to 2,250 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 221, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2,250 to 2,400 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 223, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2,400 to 2,550 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 264, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 2,550 to 2,700 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 214, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2,700 to 2,850 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 212, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2,850 to 3,000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 216, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 3,000 to 3,150 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 264, but ``max_length`` is set to 75. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 3,150 to 3,300 words\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'seconds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     end \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m.\u001b[39mseconds \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60.\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'seconds'"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "start = 0\n",
    "size = 150\n",
    "end = size\n",
    "\n",
    "start_time = datetime.datetime.utcnow()\n",
    "print(f\"Started at {start_time}\")\n",
    "for i in range(22):  # 10,570 / 500 ~= 21\n",
    "    if end > len(example1_words):\n",
    "        end = None\n",
    "    print(f\"{i+1} {start:,} to {end:,} words\")\n",
    "    text_chunk = ' '.join(example1_words[start:end]).encode(\"ascii\", errors=\"ignore\").decode().replace(\n",
    "    \"#\", \"\"\n",
    ")\n",
    "    result = summarizer(' '.join(example1_words[start:end]), max_length=75)\n",
    "    summaries.append(result[0].get('summary_text'))\n",
    "    start += size\n",
    "    end += size\n",
    "end_time = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trust you will find everything to your comfort and convenience. lady b. i don ’ t doubt it my dear lady emily, ( apart to sir b. in going out ) triumph, sir barnabas! a few more symptoms of the kind, and the day is our own. before night i ’ ll force them into an avowal of their mar¬ riage. bnz. ( going ) but, permit me, my dear — lady b. hold your tongue, sir barnabas! - ( drags him out disputing l. col. ( springs up from chair, coming down r. ) lady emily, i can ’ t con¬ ceive how a woman of your good sense, could have managed so badly as to have left these importunate and disagreeable personages even the shadow of a pretext for taking you thus by storm, * em. my dear colonel, i have told you, i did not give them even the shadow of a shade. col. at any rate, you didn ’ t want *'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/summaries.pkl\", \"wb+\") as fo:\n",
    "    pickle.dump(summaries, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each summary being limited to 75 words, we still have a large summary that we may want to reduce even further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_all = \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary_words = summary_all.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4417"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_summary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 202, but ``max_length`` is set to 20. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "first_summary = summarizer(\" \".join(all_summary_words[0:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this file was downloaded from hathitrust digital library. find more books at https : / / www. hathitrust. org. title : alarmingly suspicious, an original comedietta, in one act, by j. palgrave simpson. author : simpson, j. palgrave ( john palgrave ), 1807 - 1887. publisher : clyde, ohio, a. d. ames [ 1880? ] copyright : public domain http : / / www. hathitrust. org / access _ use # pd we have determined this work to be in the public domain, meaning that it is not subject to copyright. users are free to copy, use, and redistribute the work in part or in whole. it is possible that current copyright holders, heirs or the estate of the authors of individual portions of the work, such as illustrations or photographs, assert copyrights over these portions. depending on the nature of subsequent use that is made, additional rights may need most'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_summary[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_summary = summarizer(\" \".join(all_summary_words[500:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of Cataloging Workflow\n",
    "To illustrate a possible use of summarization, we will add a BIBFRAME Summary to the original RDF graph for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples 48\n"
     ]
    }
   ],
   "source": [
    "example1_result = requests.get(\n",
    "    \"https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c\"\n",
    ")\n",
    "example1_graph = rdflib.Graph()\n",
    "for ns, url in helpers.NAMESPACES.items():\n",
    "    example1_graph.namespace_manager.bind(ns, url)\n",
    "example1_graph.parse(\n",
    "    data=json.dumps(example1_result.json().get(\"data\")), format=\"json-ld\"\n",
    ")\n",
    "work_uri = rdflib.URIRef(\n",
    "    \"https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c\"\n",
    ")\n",
    "print(f\"Total triples {len(example1_graph)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N9e1df39e9ebd4503afdd0f6a98f7109a (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_literal = rdflib.Literal(\n",
    "    \"\"\"The Lady of Lyons, The Studio, The Vow of the Omani, The Brigands of Calabria, The Serf The Poacher's Doom, The Hunter of the Alp- Thirty-Three Next Birthday . The work is in the public domain, meaning users are free to copy, use, and redistribute the work in part or in whole .  The play is founded on incidents which actually occured during the war of the Rebellion . It introduces Ohio’s brave and gallant McPherson . It abounds with the most beautiful tableaux, drill, marches, scenes upon the battle f ^i l, in AndersonviHe .\"\"\"\n",
    ")\n",
    "summary_bnode = rdflib.BNode()\n",
    "example1_graph.add((work_uri, helpers.BIBFRAME.summary, summary_bnode))\n",
    "example1_graph.add((summary_bnode, rdflib.RDF.type, helpers.BIBFRAME.Summary))\n",
    "example1_graph.add((summary_bnode, rdflib.RDFS.label, summary_literal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix bf: <http://id.loc.gov/ontologies/bibframe/> .\n",
      "@prefix bflc: <http://id.loc.gov/ontologies/bflc/> .\n",
      "@prefix ns1: <http://www.europeana.eu/schemas/edm/> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix sinopia: <http://sinopia.io/vocabulary/> .\n",
      "\n",
      "<https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c> a bf:Work,\n",
      "        <http://share-vde.org/rdfBibframe/SuperWork> ;\n",
      "    rdfs:label \"Simpson, J. Palgrave (John Palgrave), 1807-1887. Alarmingly suspicious\"@eng ;\n",
      "    bf:adminMetadata [ a bf:AdminMetadata ;\n",
      "            bflc:catalogerId \"tt434\"@eng ;\n",
      "            bflc:encodingLevel <http://id.loc.gov/vocabulary/menclvl/f> ;\n",
      "            bf:descriptionConventions <https://id.loc.gov/vocabulary/descriptionConventions/rda> ;\n",
      "            bf:descriptionLanguage <https://id.loc.gov/vocabulary/languages/eng> ;\n",
      "            bf:descriptionModifier <https://id.loc.gov/vocabulary/organizations/cty> ;\n",
      "            bf:source <https://id.loc.gov/vocabulary/organizations/cty> ] ;\n",
      "    bf:classification [ a bf:Classification ;\n",
      "            bf:classificationPortion \"Ip Si578 1\" ;\n",
      "            bf:source <https://id.loc.gov/vocabulary/organizations/cty> ] ;\n",
      "    bf:contribution [ a bflc:PrimaryContribution ;\n",
      "            bf:agent [ a bf:Person ;\n",
      "                    owl:sameAs <http://id.loc.gov/rwo/agents/n83021558> ] ;\n",
      "            bf:role <http://id.loc.gov/vocabulary/relators/aut> ] ;\n",
      "    bf:genreForm <http://id.loc.gov/authorities/genreForms/gf2014026264>,\n",
      "        <http://id.loc.gov/authorities/genreForms/gf2014026462> ;\n",
      "    bf:language <http://id.loc.gov/vocabulary/languages/eng> ;\n",
      "    bf:note [ a bf:Note ;\n",
      "            rdfs:label \"\\\"First performed at the Royal Lyceum Theatre, Saturday, June 12th, 1852.\\\"\"@eng ] ;\n",
      "    bf:originPlace <https://id.loc.gov/rwo/agents/n79005665> ;\n",
      "    bf:summary [ a bf:Summary ;\n",
      "            rdfs:label \"The Lady of Lyons, The Studio, The Vow of the Omani, The Brigands of Calabria, The Serf The Poacher's Doom, The Hunter of the Alp- Thirty-Three Next Birthday . The work is in the public domain, meaning users are free to copy, use, and redistribute the work in part or in whole .  The play is founded on incidents which actually occured during the war of the Rebellion . It introduces Ohio’s brave and gallant McPherson . It abounds with the most beautiful tableaux, drill, marches, scenes upon the battle f ^i l, in AndersonviHe .\" ] ;\n",
      "    bf:title [ a bf:Title ;\n",
      "            bf:mainTitle \"Alarmingly suspicious\"@eng ] ;\n",
      "    sinopia:hasResourceTemplate \"Yale:RT:BF2:Monograph:SuperWork:CtY\" ;\n",
      "    ns1:occurredAt [ a ns1:TimeSpan ;\n",
      "            bf:date \"1852~\"@eng ] .\n",
      "\n",
      "<http://id.loc.gov/authorities/genreForms/gf2014026264> rdfs:label \"http://id.loc.gov/authorities/genreForms/gf2014026264\" .\n",
      "\n",
      "<http://id.loc.gov/authorities/genreForms/gf2014026462> rdfs:label \"One-act plays\" .\n",
      "\n",
      "bf:Work rdfs:label \"Work\" .\n",
      "\n",
      "<http://id.loc.gov/rwo/agents/n83021558> rdfs:label \"Simpson, J. Palgrave (John Palgrave), 1807-1887\" .\n",
      "\n",
      "<http://id.loc.gov/vocabulary/languages/eng> rdfs:label \"English\" .\n",
      "\n",
      "<http://id.loc.gov/vocabulary/menclvl/f> rdfs:label \"Full\" .\n",
      "\n",
      "<http://id.loc.gov/vocabulary/relators/aut> rdfs:label \"Author\" .\n",
      "\n",
      "<http://share-vde.org/rdfBibframe/SuperWork> rdfs:label \"http://share-vde.org/rdfBibframe/SuperWork\" .\n",
      "\n",
      "<https://id.loc.gov/rwo/agents/n79005665> rdfs:label \"https://id.loc.gov/rwo/agents/n79005665\" .\n",
      "\n",
      "<https://id.loc.gov/vocabulary/descriptionConventions/rda> rdfs:label \"rda\" .\n",
      "\n",
      "<https://id.loc.gov/vocabulary/languages/eng> rdfs:label \"English\" .\n",
      "\n",
      "<https://id.loc.gov/vocabulary/organizations/cty> rdfs:label \"CtY\",\n",
      "        \"https://id.loc.gov/vocabulary/organizations/cty\" .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example1_graph.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `stage_text_nodes`, select a series *title*, *label*, or *summary* values and compare the [spaCy][SPACY] 'en_core_web_sm' NER model with the [Huggingface][HUG] NER model results.\n",
    "\n",
    "[HUG]: https://huggingface.co/\n",
    "[SPACY]: https://spacy.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
