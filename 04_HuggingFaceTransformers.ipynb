{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers\n",
    "While spaCy provides good-enough NER cabilitities, the accelerated pace of NLP models in recent years means that we can use pre-trained models that leverage modern machine approaches. [HuggingFace](https://huggingface.co/), a company specializing in open-source models, provides an easy-to-use Python library for applying these models on text contained in Sinopia's RDF and to available full-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%reload_ext lab_black\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import kglab\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "import requests\n",
    "import helpers\n",
    "import widgets\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sinopia Stage RDF Text DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes = pd.read_json(\"data/stage-text-nodes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface NER Pipeline\n",
    "The HuggingFace transformers library provides very easy-to-use pipelines for running common NLP tasks like NER. We will create a NER pipeline and run the following *summary* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stage_text_nodes.iloc[3566].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = ner_pipe(stage_text_nodes.iloc[3566].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in ner_results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results returned from the [HuggingFace][HUG] NER contain more information and are structured differently than [spaCy][SPACY]. In the [HuggingFace][HUG] pipeline, there are only four classes of entities:\n",
    "\n",
    "- `I-PER` for a Person name\n",
    "- `I-ORG` for an Organization name\n",
    "- `I-LOC` for a location\n",
    "- `I-MISC` for a Miscellaneous entity. \n",
    "\n",
    "The HuggingFace NER also gives a statistical score on how the model is confident that it matched an entity. Also, the [HuggingFace][HUG] NER pipeline results does some character masking (seen as `##` in the results) for many of the entities.\n",
    "\n",
    "[HUG]: https://huggingface.co/\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Summarization Pipeline\n",
    "Another [HuggingFace][HUG] pipeline is the *summarization* task that takes a large document and automatically summarizes the text. The pipeline leverages a [Bart](https://arxiv.org/abs/1910.13461) model that was fine-tuned on a CNN / Daily Mail data set.\n",
    "\n",
    "[HUG]: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we looked a *Alarmingly suspicious* that is cataloged in Sinopia at https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c. This resource does not have an abstract or BIBFRAME Summary, so let us read in sections of the full-text to this [HuggingFace][HUG] summizer and see if can autogenerate a summary and add it to this RDF graph.\n",
    "\n",
    "[HUG]: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/65a2b059-5ac1-48a6-adbb-870712c3060c.txt\") as fo:\n",
    "    example1_text = fo.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we send the entire full-text to the `summarizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_result = summarizer(example1_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of tokens that the `summarizer` pipeline can process at a time is **1024** while our full-text has **20,348** tokens. Let us try breaking down our large text into smaller \"chunks\", send each chunk into the `summarizer` pipeline, capture the resulting summary, and at the end, see if the summaries make sense.\n",
    "\n",
    "First, we will create a list of all of the words in the full-text and then send and summarize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_words = example1_text.split()\n",
    "print(f\"Total words in full-text {len(example1_words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries = []\n",
    "start = 0\n",
    "size = 500\n",
    "end = size\n",
    "\n",
    "start_time = datetime.datetime.utcnow()\n",
    "print(f\"Started at {start_time}\")\n",
    "for i in range(22):  # 10,570 / 500 ~= 21\n",
    "    if end > len(example1_words):\n",
    "        end = None\n",
    "    print(f\"{i+1} {start:,} to {end:,} words\")\n",
    "    text_chunk = ' '.join(example1_words[start:end]).encode(\"ascii\", errors=\"ignore\").decode().replace(\n",
    "    \"#\", \"\"\n",
    ")\n",
    "    result = summarizer(' '.join(example1_words[start:end]), max_length=75)\n",
    "    summaries.append(result[0].get('summary_text'))\n",
    "    start += size\n",
    "    end += size\n",
    "end_time = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/summaries.pkl\", \"wb+\") as fo:\n",
    "    pickle.dump(summaries, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each summary being limited to 75 words, we still have a large summary that we may want to reduce even further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_all = \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary_words = summary_all.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer(\" \".join(all_summary_words[0:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer(\" \".join(all_summary_words[500:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of Cataloging Workflow\n",
    "To illustrate a possible use of summarization, we will add a BIBFRAME Summary to the original RDF graph for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_result = requests.get(\n",
    "    \"https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c\"\n",
    ")\n",
    "example1_graph = rdflib.Graph()\n",
    "for ns, url in helpers.NAMESPACES.items():\n",
    "    example1_graph.namespace_manager.bind(ns, url)\n",
    "example1_graph.parse(\n",
    "    data=json.dumps(example1_result.json().get(\"data\")), format=\"json-ld\"\n",
    ")\n",
    "work_uri = rdflib.URIRef(\n",
    "    \"https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c\"\n",
    ")\n",
    "print(f\"Total triples {len(example1_graph)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_literal = rdflib.Literal(\n",
    "    \"\"\"The Lady of Lyons, The Studio, The Vow of the Omani, The Brigands of Calabria, The Serf The Poacher's Doom, The Hunter of the Alp- Thirty-Three Next Birthday . The work is in the public domain, meaning users are free to copy, use, and redistribute the work in part or in whole. The play is founded on incidents which actually occured during the war of the Rebellion . It introduces Ohioâ€™s brave and gallant McPherson . It abounds with the most beautiful tableaux, drill, marches, scenes upon the battle, in AndersonviHe .\"\"\"\n",
    ")\n",
    "summary_bnode = rdflib.BNode()\n",
    "example1_graph.add((work_uri, helpers.BIBFRAME.summary, summary_bnode))\n",
    "example1_graph.add((summary_bnode, rdflib.RDF.type, helpers.BIBFRAME.Summary))\n",
    "example1_graph.add((summary_bnode, rdflib.RDFS.label, summary_literal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example1_graph.serialize(format=\"turtle\").decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `stage_text_nodes`, select a series *title*, *label*, or *summary* values and compare the [spaCy][SPACY] 'en_core_web_sm' NER model with the [Huggingface][HUG] NER model results.\n",
    "\n",
    "[HUG]: https://huggingface.co/\n",
    "[SPACY]: https://spacy.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
