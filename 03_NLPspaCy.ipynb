{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) with spaCy\n",
    "In this notebook, we will use the Natural Language Processing library for Python called [spaCy][SPACY]. \n",
    "The functionality provided by [spaCy][SPACY] allows us to quickly extract parts-of-speech (POS) from text descriptions and to identify entities using [spaCy's][SPACY] named entity recognition (NER). We will initially  use [spaCy][SPACY]'s rule-based matching functionality to create a subject matcher that we can then apply to both Sinopia's RDF metadata as well as the associated full-text for a select sample of these resources.\n",
    "\n",
    "\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%reload_ext lab_black\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import string\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "import kglab\n",
    "import rdflib\n",
    "import helpers\n",
    "import requests\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a FAST Panda DataFrame\n",
    "Using a csv file derived from OCLC's [FAST](https://www.oclc.org/research/areas/data-science/fast.html)(Faceted Application of Subject Terminology) topic list, we read this csv into a Panda's DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_topics = pd.read_csv(\"data/topic_uri_label_utf8.csv\", names=[\"URL\", \"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what data is in the `fast_topics` DataFrame we can look at the *shape*, *info*, and look at a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of fast_topics {fast_topics.shape}\")\n",
    "fast_topics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_topics.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our sample, we see that the `name` has punctuation like **(),--** that will need to be removed later in our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Phrase Matcher\n",
    "With the [FAST][FAST] dataframe as our source, we now will create a [spaCy][SPACY] [PhraseMatcher](https://spacy.io/usage/rule-based-matching#phrasematcher) made up of phrases extracted from the dataframe using the [FAST][FAST] urls as identifiers.\n",
    "\n",
    "We start by importing [spaCy][SPACY] base English vocabulary and create an empty natural language processing (nlp) pipeline that we pass into a new matcher object that will lower-case all of the [FAST][FAST] patterns.\n",
    "\n",
    "[FAST]: https://www.oclc.org/research/areas/data-science/fast.html\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "fast_topic_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"African American teenagers--Education\".strip(string.punctuation).replace(\n",
    "    \"--\", \" \"\n",
    ").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatings Patterns\n",
    "We will iterate through the dataframe and for each [FAST](https://www.oclc.org/research/areas/data-science/fast.html) series, using the **URL** as the identififer and for the name, remove punctuation, split each name into a list of words, and tokenize each term, and then add to the `fast_topic_matcher`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.utcnow()\n",
    "print(f\"Started at {start} of FAST Matching\")\n",
    "# Iterate through the FAST Topic DataFrame\n",
    "for row in fast_topics.iterrows():\n",
    "    ident = row[1][\"URL\"]\n",
    "    name_list = row[1][\"name\"].strip(string.punctuation).replace(\"--\", \" \").split()\n",
    "    name_list = [nlp.make_doc(row) for row in name_list]\n",
    "    fast_topic_matcher.add(ident, name_list)\n",
    "end = datetime.datetime.utcnow()\n",
    "print(\n",
    "    f\"Finished adding FAST Topic patterns at {end}, total time {(end-start).seconds / 60.}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the spaCy Phrase matcher to titles and summaries from the Sinopia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sinopia Stage Knowledge Graph\n",
    "Just like in the previous Jupyter notebook, we will load the saved knowledge graph that we created at the beginning and then we will query the graph using SPARQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_kg = kglab.KnowledgeGraph()\n",
    "stage_kg.load_jsonld(\"data/stage.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDF Literals Pandas DataFrame\n",
    "With the Sinopia Stage knowledge graph loaded, we will query our Graph for *titles*, *labels*, and any *summary* triples and apply our `fast_topic_matcher` to the results.\n",
    "\n",
    "> **NOTE** The `stage_text_nodes` dataframe has been saved in the **data** directory \n",
    "> and can be loaded for use instead of running this SPARQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes = stage_kg.query_as_df(\n",
    "    \"\"\"PREFIX bf: <http://id.loc.gov/ontologies/bibframe/>\n",
    "\n",
    "SELECT ?title ?label ?summary ?url\n",
    "\n",
    "WHERE {\n",
    "  OPTIONAL {\n",
    "      ?title_bnode  bf:mainTitle ?title .\n",
    "      ?url bf:title ?title_bnode .\n",
    "  }\n",
    "  OPTIONAL {\n",
    "      ?label_bnode rdfs:label ?label .\n",
    "      ?url bf:title ?label_bnode .\n",
    "  }\n",
    "  OPTIONAl {\n",
    "      ?url bf:summary ?summary_bnode .\n",
    "      ?summary_bnode rdfs:label ?summary  .\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes.to_json(\"data/stage-text-nodes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_text_nodes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(stage_text_nodes.iloc[11484].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(fast_topic_matcher(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_match = fast_topic_matcher(doc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings[first_match[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_topics[fast_topics[\"URL\"] == \"http://id.worldcat.org/fast/869075\"].iloc[0][\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_11484 = helpers.get_matches(\n",
    "    stage_text_nodes.iloc[11484].title, nlp, fast_topic_matcher, fast_topics, 25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matches_11484"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [spaCy][SPACY] `fast_topic_matcher` is greedy, meaning that it matches on any terms contained in the document as we can see in this example. \n",
    "\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_11500 = helpers.get_matches(\n",
    "    stage_text_nodes.iloc[11500].title, nlp, fast_topic_matcher, fast_topics, 25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_11500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Named Entity Recognition (NER)\n",
    "One of the strengths of [spaCy][SPACY] is it's pre-existing models for named entity recognition (NER). In NER tasks, existing entities like people, locations, time periods are identified and tagged for later analysis. \n",
    "\n",
    "To demonstrate, we will load a small English language model and perform NER on some of the *titles*, *labels*, and any *summary* triples contained in the `stage_text_nodes` dataframe.\n",
    "\n",
    "### Download model and Create NLP pipeline\n",
    "We first need to download the `en_core_web_sm` model for our environment and then create an instance of a NLP pipeline.\n",
    "\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Documents and Display Entities\n",
    "Using a different sample from the `stage_text_nodes` dataframe, we apply the `en_nlp` pipeline to some of these values to see if we can extract meaningful entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = stage_text_nodes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_10315 = en_nlp(stage_text_nodes.iloc[10315].title)\n",
    "doc_3087 = en_nlp(stage_text_nodes.iloc[3087].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entities for 10315\")\n",
    "for ent in doc_10315.ents:\n",
    "    print(ent, ent.label_)\n",
    "print(\"\\n\\nEntities for 3087\")\n",
    "for ent in doc_3087.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy NER and POS Visualizations\n",
    "The [spaCy][SPACY] project provides the `displacy` class that has two helpful visualizers. The first **ner** displays the identified entities in context of the text and the second identifies the parts-of-speech and how they are related to each other in the **dep** option. \n",
    "\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc_3087, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc_10315, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Download and create a `nlp` pipeline using the *en_core_web_sm* model and see what entities are identified using your sample from the `stage_text_nodes` dataframe.\n",
    "\n",
    "> **Extra credit** Find a non-english resource in `stage_text_nodes`, find a corresponding \n",
    "> [spaCy][SPACY] trained pipeline at https://spacy.io/models and follow the above steps to \n",
    "> identify any entities in the *title*, *label*, or *summary* values.\n",
    "\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Text and Sinopia RDF Metadata\n",
    "Exciting possibilities of applying machine learning to library workflows comes when we combine linked-data cataloging with the availability of the full-text. For example, we can run NER on the entirety of the full-text, and add any identified entities to our RDF. \n",
    "\n",
    "> Special thanks to Tim Thompson and Greta de Groat for providing examples of cataloged\n",
    "> resources in Sinopia that have available full-text!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example One: *Alarmingly suspicious*\n",
    "First we will download the Sinopia record for this Work, https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c, and extract the `data` property to create a RDF graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_result = requests.get(\n",
    "    \"https://api.sinopia.io/resource/65a2b059-5ac1-48a6-adbb-870712c3060c\"\n",
    ")\n",
    "example1_graph = rdflib.Graph()\n",
    "for ns, url in helpers.NAMESPACES.items():\n",
    "    example1_graph.namespace_manager.bind(ns, url)\n",
    "example1_graph.parse(\n",
    "    data=json.dumps(example1_result.json().get(\"data\")), format=\"json-ld\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we open the full-text that was retrieved from the [Haithtrust](https://babel.hathitrust.org/cgi/pt?id=uiuo.ark:/13960/t14n5kz7q&view=1up&seq=3) and load all of the text and save to the `example1_text` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/65a2b059-5ac1-48a6-adbb-870712c3060c.txt\") as fo:\n",
    "    example1_text = fo.read()\n",
    "\n",
    "print(f\"Number of characters in the text is {len(example1_text):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we will run the full-text through our [spaCy][SPACY] en_nlp pipeline and then examine the identified entities.\n",
    "\n",
    "[SPACY]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_doc = en_nlp(example1_text)\n",
    "print(f\"Number of entities: {len(example1_doc.ents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 576 entities, we will extract the **PERSON** and **GPE** (location) entities for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people, locations = [], []\n",
    "\n",
    "for ent in example1_doc.ents:\n",
    "    if ent.label_.startswith(\"PERSON\"):\n",
    "        people.append(ent)\n",
    "    if ent.label_.startswith(\"GPE\"):\n",
    "        locations.append(ent)\n",
    "print(f\"NER found {len(people):,} people and {len(locations):,} locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `people` list, we notice a number of duplications (for now we don't care where the person entity was found in the text) so we filter this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = set([\" \".join(ent.text.split()) for ent in people])\n",
    "locations = set([\" \".join(ent.text.split()) for ent in locations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(people), len(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these identified names, we can use the VIAF [SRU Search](https://platform.worldcat.org/api-explorer/apis/VIAF/AuthorityCluster/SRUSearch) on each of the `people` entities to see if we can narrow down our list using this authority source. In the helpers module, the `viaf_people_search` function returns a dictionary VIAF urls that match the term and the first main heading of the record.\n",
    "\n",
    "We can then aggregate these results and see what remains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_people = {}\n",
    "for i, term in enumerate(list(people)):\n",
    "    # Removes newlines, tabs, and spaces from the entity text\n",
    "    viaf_result = helpers.viaf_people_search(term)\n",
    "    example1_people.update(viaf_result)\n",
    "    if not i % 10 and i > 0:\n",
    "        print(\".\", end=\"\")\n",
    "    if not i % 25:\n",
    "        print(f\"{i}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example1_people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 - *Seismic performance assessment of buildings*\n",
    "The second example is the report, *Seismic performance assessment of buildingss*, described by the following Sinopia Resources:\n",
    "\n",
    "- BIBFRAME Work https://api.stage.sinopia.io/resource/2ffc86d1-4850-4e4b-974c-49ded8ce4b3f\n",
    "- BIBFRAME Instance https://api.stage.sinopia.io/resource/e865acc1-9b24-401e-a0eb-36ca1ca8b1d6\n",
    "\n",
    "\n",
    "First we will retrieve these resources and add them to a RDF graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of triples for example 2 152\n"
     ]
    }
   ],
   "source": [
    "example2_graph = rdflib.Graph()\n",
    "for ns, url in helpers.NAMESPACES.items():\n",
    "    example2_graph.namespace_manager.bind(ns, url)\n",
    "\n",
    "for sinopia_url in [\n",
    "    \"https://api.stage.sinopia.io/resource/2ffc86d1-4850-4e4b-974c-49ded8ce4b3f\",\n",
    "    \"https://api.stage.sinopia.io/resource/e865acc1-9b24-401e-a0eb-36ca1ca8b1d6\",\n",
    "]:\n",
    "    sinopia_result = requests.get(sinopia_url)\n",
    "    example2_graph.parse(\n",
    "        data=json.dumps(sinopia_result.json()[\"data\"]), format=\"json-ld\"\n",
    "    )\n",
    "print(f\"Total number of triples for example 2 {len(example2_graph):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will open and read the full-text for *Improving water supply networks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the text is 671,016\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/e865acc1-9b24-401e-a0eb-36ca1ca8b1d6.txt\") as fo:\n",
    "    example2_text = fo.read()\n",
    "\n",
    "print(f\"Number of characters in the text is {len(example2_text):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we will run the full-text through our [spaCy][SPACY] en_nlp pipeline and then examine the identified entities.\n",
    "\n",
    "[SPACY]: https://spacy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 10,220\n"
     ]
    }
   ],
   "source": [
    "example2_doc = en_nlp(example2_text)\n",
    "print(f\"Number of entities: {len(example2_doc.ents):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_types = {}\n",
    "for ent in example2_doc.ents:\n",
    "    if ent.label_ in ent_types:\n",
    "        ent_types[ent.label_] += 1\n",
    "    else:\n",
    "        ent_types[ent.label_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ORG': 1820,\n",
       " 'WORK_OF_ART': 324,\n",
       " 'PRODUCT': 672,\n",
       " 'CARDINAL': 4170,\n",
       " 'GPE': 686,\n",
       " 'PERSON': 685,\n",
       " 'FAC': 42,\n",
       " 'DATE': 533,\n",
       " 'MONEY': 136,\n",
       " 'ORDINAL': 138,\n",
       " 'NORP': 34,\n",
       " 'QUANTITY': 36,\n",
       " 'PERCENT': 794,\n",
       " 'LAW': 61,\n",
       " 'LOC': 22,\n",
       " 'TIME': 62,\n",
       " 'EVENT': 5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER found 685 people and 686 locations\n"
     ]
    }
   ],
   "source": [
    "example2_people, example2_locations = [], []\n",
    "\n",
    "for ent in example2_doc.ents:\n",
    "    if ent.label_.startswith(\"PERSON\"):\n",
    "        example2_people.append(ent)\n",
    "    if ent.label_.startswith(\"GPE\"):\n",
    "        example2_locations.append(ent)\n",
    "print(\n",
    "    f\"NER found {len(example2_people):,} people and {len(example2_locations):,} locations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639, 641)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example2_people[90].start, example2_people[90].end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_people[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now de-duplicate `example2_people` and `example2_locations`example2_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_people = set([\" \".join(ent.text.split()) for ent in example2_people])\n",
    "example2_locations = set([\" \".join(ent.text.split()) for ent in example2_locations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example2_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(example2_people)[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russell Larsen\n",
      "Udit S.\n",
      "Khalid Mosalam\n",
      "-1.204\n",
      "John Wallace\n",
      "gm β\n",
      "Yin-Nan Huang\n",
      "C-3\n",
      "Christopher Rojahn\n",
      "Travis Chrupalo\n",
      "Dan Gramer\n",
      "Table G-2\n",
      "Haselton Baker Risk Group\n",
      "+1\n",
      "Steven McCabe Morley Builders\n",
      "Atkinson\n",
      "Mohamed M. Talaat\n",
      "Peter J. May\n",
      "Continuous Distributions\n",
      "β2 \n",
      "Harmsen\n",
      "Pampanin et al.\n",
      "Appendix D.\n",
      "K-1 Overturning\n",
      "Erica Hays\n",
      "Appendix G\n",
      "Monte Carlo\n",
      "David R. Bonneville\n",
      "Steve Mahin\n",
      "W\n",
      "Table H-1\n",
      "B.1\n",
      "Farzad Naeim\n",
      "max min R\n",
      "Bruce R. Ellingwood\n",
      "Exterior Walls\n",
      "John Hooper\n",
      "Appendix K:\n",
      "Jennifer Tobin-Gurley\n",
      "r.\n",
      "B.\n",
      "W.\n",
      "Katherine Wade\n",
      "i.\n",
      "Stephen A. Mahin\n",
      "Gayle Johnson\n",
      "Jack Baker\n",
      "William O’Brien\n",
      "David Bonneville\n",
      "Appendix E. Population\n"
     ]
    }
   ],
   "source": [
    "for person in list(example2_people)[50:100]:\n",
    "    print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example2_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R.O.\n",
      "Tehran\n",
      "I-1\n",
      "Vairo\n",
      "D.C.\n",
      "SF\n",
      "B.7\n",
      "L.\n",
      "T.Y.\n",
      "Bozorgnia\n",
      "Western United States\n",
      "Oakland\n",
      "Medina\n",
      "D.M.\n",
      "South Carolina\n",
      "Yang\n",
      "Michigan\n",
      "≤T ≤T\n",
      "G.\n",
      "C.A.\n",
      "lnS −lnθ\n",
      "7.0E-05\n",
      "Chesterfield\n",
      "the United States\n",
      "S.R.\n",
      "Normal Distributions\n",
      "Whittaker\n",
      "P.G.\n",
      "Oregon\n",
      "Mill Valley\n",
      "MC0085\n",
      "Virginia\n",
      "San Mateo\n",
      "V.V.\n",
      "Reston\n",
      "Newport Beach\n",
      "Stockton\n",
      "Iran\n",
      "Carson\n",
      "Washington\n",
      "H.K.\n",
      "Cherry Street\n",
      "Redwood City\n",
      "Loads\n",
      "W.J.\n",
      "Dublin\n",
      "Escondido\n",
      "Γ\n",
      "Fragilities\n",
      "Sequential\n"
     ]
    }
   ],
   "source": [
    "for location in list(example2_locations)[0:50]:\n",
    "    print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "For the following Sinopia Resources, create a graph, and a NLP pipeline to identify any people and locations in the full-text. The full-text is available in `data/a90f911e-d03e-427f-a36e-ed1e3d1a9b3b.txt` file.\n",
    "\n",
    "- BIBFRAME Work https://api.stage.sinopia.io/resource/d1e377d0-9ca6-4377-a9a1-a20303c5fe66\n",
    "- BIBFRAME Instance https://api.stage.sinopia.io/resource/a90f911e-d03e-427f-a36e-ed1e3d1a9b3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
